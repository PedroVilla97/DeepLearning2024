{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -U tifffile[all]\n",
    "# !pip list\n",
    "# Install dependencies\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a446796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 18:17:43.807268: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-21 18:17:43.850460: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 18:17:44.016297: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-21 18:17:44.017215: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-21 18:17:44.828183: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, Model\n",
    "from osgeo import gdal\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4d7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_file):\n",
    "    with open(json_file, 'r') as file: \n",
    "        data = json.load(file)\n",
    "    return data \n",
    "\n",
    "data_s1 = load_json_data('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/S1list.json')\n",
    "data_s2 = load_json_data('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/S2list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed791fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load images as we saw on kaggle\n",
    "# img = tiff.imread('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/0066/S2_2019-02-04_B01.tif')\n",
    "# img_array = np.array(img)\n",
    "# plt.imshow(img)\n",
    "# print(img_array.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4dba727",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is for read the tiff images using the tiff python library\n",
    "### Images, filenames and flooding labels are set to be stored in arrays while reading the images\n",
    "### Function takes a particular directory (A Flloding scenario and fetch for each label in the flooding json files)\n",
    "### It automatically fetch for the number of the folder being processed \n",
    "### And retrieve both the name of the file and the flooding label. e.g from the sequence:\n",
    "### \n",
    "###\n",
    "### {\"0063\": {\"1\": {\"date\": \"2019-02-04\", \"FLOODING\": false, \"FULL-DATA-COVERAGE\": true, \"filename\": \"S2_2019-02-04\"}, \"count\": 1, \"folder\": \"0063\", \"geo\": {\"type\": \"Polygon\", \"coordinates\": [[[28.29722, -15.382762], [28.297507, -15.429039], [28.345216, -15.428755], [28.344918, -15.382479], [28.29722, -15.382762]]]}},\n",
    "### Based on the name of the folder \"0063\" it reads the images of the folder and it appends both filename and flooding label.\n",
    "### To cross and validate whether there is flooding or not the folder \"0200\" is used on the next cell. \n",
    "### This is the second sequence in the S2 json data. Easy to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory, s1_data, s2_data):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    \n",
    "    folder_name = os.path.basename(directory)\n",
    "    print(f\"Processing folder: {folder_name}\")\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.tif') or file_name.endswith('.tiff'):\n",
    "            json_data = s1_data if file_name.startswith('S1') else s2_data if file_name.startswith('S2') else None\n",
    "            if json_data is None: \n",
    "                print(f\"Skipping file (not S1 or S2): {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            found = False\n",
    "            for item in json_data.get(folder_name, {}).values():\n",
    "                if isinstance(item, dict) and 'filename' in item and item['filename'] in file_name:\n",
    "                    file_path = os.path.join(directory, file_name)\n",
    "                    img = tiff.imread(file_path)\n",
    "                    img_array = np.array(img)\n",
    "                    images.append(img_array)\n",
    "                    filenames.append(file_name)\n",
    "                    labels.append(item.get('FLOODING', False))\n",
    "                    found = True \n",
    "                    break\n",
    "    return images, filenames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdbbba48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4701e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Following code is to get a list of all the flooding events (folders)\n",
    "### present on the daataset. There is also code to select which folders are\n",
    "### going to be selected to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "51fb7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This code is to apply recursively the function to read the images to all \n",
    "### The folders present on the dataset. As the number of the sequence event is stored in a \n",
    "### dictionary everythins is stored on the same dictionary with sequence and flooding label\n",
    "\n",
    "def process_folders(folder_list, main_directory, s1_data, s2_data):\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_filenames = []\n",
    "    scenario_labels = []\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        directory = os.path.join(main_directory, folder)\n",
    "        images, filenames, labels = load_images_from_directory(directory, s1_data, s2_data)\n",
    "        \n",
    "        all_images.extend(images)\n",
    "        all_labels.extend(labels)\n",
    "        all_filenames.extend(filenames)\n",
    "        scenario_labels.extend([folder] * len(labels))\n",
    "        \n",
    "    return all_images, all_labels, all_filenames, scenario_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "958d7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(main_directory):\n",
    "    return [f for f in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "# Main SEN12FLOOD directory, whole data is there\n",
    "main_directory = '/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD'\n",
    "\n",
    "# List all folders\n",
    "all_folders = list_folders(main_directory)\n",
    "\n",
    "# Manually select folders for training and testing\n",
    "# test_folders = [str(i) for i in range(68)]  # Example folders for training\n",
    "# train_folders = ['0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0016', '0018', '0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029', '0030', '0031', '0033', '0034', '0035', '0036', '0037', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0050', '0053', '0054', '0055', '0057', '0059', '0060', '0061', '0063', '0065', '0066', '0067', '0068', '0069', '0070', '0071', '0072', '0073', '0074', '0075', '0076', '0077', '0079', '0080', '0081', '0082', '0084', '0085', '0086', '0088', '0089', '0090', '0091', '0093', '0094', '0095', '0096', '0097', '0098', '0099', '0100', '0101', '0102', '0103', '0104', '0105', '0106', '0107', '0108', '0109', '0111', '0115', '0116', '0117', '0118', '0120', '0121', '0122', '0123', '0124', '0125', '0126', '0127', '0128', '0130', '0131', '0132', '0133', '0134', '0135', '0137', '0138', '0139', '0140', '0141', '0143', '0144', '0145', '0146', '0147', '0148', '0149', '0150', '0151', '0154', '0155', '0156', '0157', '0158', '0159', '0160', '0161', '0162', '0163', '0165', '0166', '0167', '0168', '0169', '0170', '0171', '0173', '0174', '0176', '0177', '0178', '0181', '0182', '0184', '0186', '0187', '0188', '0191', '0192', '0193', '0194', '0196', '0198', '0199', '0200', '0201', '0203', '0204', '0205', '0206', '0207', '0208', '0209', '0210', '0212', '0213', '0214', '0215', '0216', '0217', '0218', '0219', '0220', '0221', '0222', '0223', '0225', '0226', '0227', ' 0229', '0230', '0231', '0232', '0233', '0234', '0235', '0236', '0238', '0240', '0241', '0243', '0244', '0245', '0246', '0247', '0248', '0249', '0250', '0253', '0254', '0255', '0256', '0257', '0258', '0259', '0260', '0261', '0262', '0263', '0266', '0267', '0271', '0272', '0273', '0274', '0275', '0276', '0277', '0278', '0279', '0280', '0281', '0282', '0285', '0286', '0287', '0288', '0290', '0293', '0294', '0295', '0296', '0298', '0299', '0300', '0301', '0303', ' 0304', '0305', '0306', '0307', '0308', '0309', '0310', '0311', '0313', '0316', '0318', '0319', '0320', '0321', '0323', '0324', '0325', '0326', '0327', '0328', '0329', '0330', '0331', '0332', '0333', '0334', '0335', '0336']   # Example folders for testing\n",
    "train_folders = ['0200', '0001', '0002']\n",
    "test_folders = ['26']\n",
    "validation_folders = ['61']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7c77b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEPNG THE CODE BELOW. DISCUSS IT WITH PEDRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f3436db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Preprocess train and test folders\n",
    "# train_images, train_labels, train_scenario_labels = process_folders(train_folders, main_directory, data_s1, data_s2)\n",
    "# test_images, test_labels, test_scenario_labels = process_folders(test_folders, main_directory, data_s1, data_s2)\n",
    "# validation_images, validation_labels, validation_scenario_labels = process_folders(validation_folders, main_directory, data_s1, data_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa4e50e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 0200\n",
      "Processing folder: 0001\n",
      "Processing folder: 0002\n",
      "Processing folder: 26\n",
      "Processing folder: 61\n"
     ]
    }
   ],
   "source": [
    "### Preprocess train and test folders\n",
    "train_images, train_labels, train_filenames, train_scenarios = process_folders(train_folders, main_directory, data_s1, data_s2)\n",
    "test_images, test_labels, test_filenames, test_scenarios = process_folders(test_folders, main_directory, data_s1, data_s2)\n",
    "validation_images, validation_labels, validation_filenames, validation_scenarios = process_folders(validation_folders, main_directory, data_s1, data_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d08c709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There is one missing thing here.\n",
    "# We can parse mutiple folders, and collect the images and labels. However, we are still missing to keep track individual files\n",
    "# That is important later for printing the images. So the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "536bf882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data sample:\n",
      "Filename train: S1A_IW_GRDH_1SDV_20190412T031706_20190412T031731_026751_030125_194F_corrected_VV.tif, Flooding: True, Scenario: 0200\n",
      "Filename train: S2_2019-03-25_B12.tif, Flooding: True, Scenario: 0200\n",
      "Filename train: S1A_IW_GRDH_1SDV_20190318T162351_20190318T162420_026394_02F415_5335_corrected_VH.tif, Flooding: True, Scenario: 0200\n",
      "Filename train: S1A_IW_GRDH_1SDV_20190331T031706_20190331T031731_026576_02FABC_DD51_corrected_VH.tif, Flooding: True, Scenario: 0200\n",
      "Filename train: S2_2019-03-10_B09.tif, Flooding: False, Scenario: 0200\n",
      "\n",
      "Validation data sample:\n",
      "Filename validation: S2_2019-02-13_B05.tif, Flooding: False, Scenario: 61\n",
      "Filename validation: S1A_IW_GRDH_1SDV_20190412T031706_20190412T031731_026751_030125_194F_corrected_VV.tif, Flooding: True, Scenario: 61\n",
      "Filename validation: S2_2019-03-25_B12.tif, Flooding: True, Scenario: 61\n",
      "Filename validation: S1A_IW_GRDH_1SDV_20190318T162351_20190318T162420_026394_02F415_5335_corrected_VH.tif, Flooding: True, Scenario: 61\n",
      "Filename validation: S1A_IW_GRDH_1SDV_20190331T031706_20190331T031731_026576_02FABC_DD51_corrected_VH.tif, Flooding: True, Scenario: 61\n",
      "\n",
      "Test data sample:\n",
      "Filename test: S2_2018-12-28_B05.tif, Flooding: False, Scenario: 26\n",
      "Filename test: S2_2018-12-28_B07.tif, Flooding: False, Scenario: 26\n",
      "Filename test: S2_2019-01-02_B07.tif, Flooding: False, Scenario: 26\n",
      "Filename test: S2_2019-02-01_B05.tif, Flooding: False, Scenario: 26\n",
      "Filename test: S2_2019-01-02_B08.tif, Flooding: False, Scenario: 26\n",
      "\n",
      "Shape of datasets:\n",
      "Train Images Shape: (240, 522, 544)\n",
      "Test Images Shape: (42, 522, 544)\n",
      "Validation Images Shape: (138, 522, 544)\n",
      "Train Labels Shape: (240,)\n",
      "Test Labels Shape: (42,)\n",
      "Validation Labels Shape: (138,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.transform import resize\n",
    "\n",
    "# Define the target shape for resizing images\n",
    "target_shape = (522, 544)  # Adjust as needed\n",
    "\n",
    "# Assuming you have lists of images from previous steps\n",
    "resized_train_images = [resize(image, target_shape, preserve_range=True) for image in train_images]\n",
    "resized_test_images = [resize(image, target_shape, preserve_range=True) for image in test_images]\n",
    "resized_validation_images = [resize(image, target_shape, preserve_range=True) for image in validation_images]\n",
    "\n",
    "# Convert the resized images and labels into numpy arrays for use in the model\n",
    "X_train = np.array(resized_train_images)\n",
    "X_test = np.array(resized_test_images)\n",
    "X_val = np.array(resized_validation_images)\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "y_val = np.array(validation_labels)\n",
    "\n",
    "# Print sample data from training set to check structure\n",
    "print(\"Training data sample:\")\n",
    "for filename, label, scenario in zip(train_filenames[:5], train_labels[:5], train_scenarios[:5]):\n",
    "    print(f\"Filename train: {filename}, Flooding: {label}, Scenario: {scenario}\")\n",
    "\n",
    "# Include scenario labels in the validation and test set print statements\n",
    "print(\"\\nValidation data sample:\")\n",
    "for filename, label, scenario in zip(validation_filenames[:5], validation_labels[:5], validation_scenarios[:5]):\n",
    "    print(f\"Filename validation: {filename}, Flooding: {label}, Scenario: {scenario}\")\n",
    "\n",
    "print(\"\\nTest data sample:\")\n",
    "for filename, label, scenario in zip(test_filenames[:5], test_labels[:5], test_scenarios[:5]):\n",
    "    print(f\"Filename test: {filename}, Flooding: {label}, Scenario: {scenario}\")\n",
    "\n",
    "# Additional debug prints to check the shape of the arrays\n",
    "print(\"\\nShape of datasets:\")\n",
    "print(\"Train Images Shape:\", X_train.shape)\n",
    "print(\"Test Images Shape:\", X_test.shape)\n",
    "print(\"Validation Images Shape:\", X_val.shape)\n",
    "print(\"Train Labels Shape:\", y_train.shape)\n",
    "print(\"Test Labels Shape:\", y_test.shape)\n",
    "print(\"Validation Labels Shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7dd5cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Structure the data to prepare the input for the ConvLSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af296a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import defaultdict \n",
    "\n",
    "def group_by_scenario_and_type(images, labels, scenarios, filenames):\n",
    "    grouped_data = defaultdict(lambda: {'S1': [], 'S2': [], 'labels': []})\n",
    "    for img, lbl, scn, fname in zip(images, labels, scenarios, filenames):\n",
    "        if 'S1' in fname:\n",
    "            grouped_data[scn]['S1'].append(img)\n",
    "        elif 'S2' in fname:\n",
    "            grouped_data[scn]['S2'].append(img)\n",
    "        grouped_data[scn]['labels'].append(lbl)  # Assuming labels are common and not split by type\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67ef7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = group_by_scenario_and_type(resized_train_images, train_labels, train_scenarios, train_filenames)\n",
    "test_grouped = group_by_scenario_and_type(resized_test_images, test_labels, test_scenarios, test_filenames)\n",
    "validation_grouped = group_by_scenario_and_type(resized_validation_images, validation_labels, validation_scenarios, validation_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "243f2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_for_conv_lstm(grouped_data):\n",
    "    X_s1, X_s2, y = [], [], []\n",
    "    for scenario in grouped_data:\n",
    "        scenario_images_s1 = np.array(grouped_data[scenario]['S1'])\n",
    "        scenario_images_s2 = np.array(grouped_data[scenario]['S2'])\n",
    "        scenario_labels = np.array(grouped_data[scenario]['labels'])  # Adjusted to 'labels'\n",
    "        \n",
    "        if len(scenario_images_s1) > 0 and len(scenario_images_s2) > 0:  # Ensure both types have images\n",
    "            X_s1.append(scenario_images_s1)\n",
    "            X_s2.append(scenario_images_s2)\n",
    "            y.append(scenario_labels[0])  # Taking the first label as an example; adjust as needed based on your labeling strategy\n",
    "    return np.array(X_s1), np.array(X_s2), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "045061d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Erase the previous model. Strategy must be to train, test and validate model \n",
    "#### By sequences and not by all te images not considering the scenario (folder) label\n",
    "\n",
    "#### After consultating with my doctor he has recommend me to go with the \n",
    "### Convolutional LSTM as it is convenient for time-series works\n",
    "### And also can work multidimensionally which make it a good fit for image and video analysis\n",
    "\n",
    "#### Define the model\n",
    "\n",
    "# Structure Data Appropriately: Each input for the ConvLSTM needs to be shaped as (samples, time_steps, height, width, channels), where samples is the number of sequences, time_steps is the number of images in each sequence, height and width are the dimensions of each image, and channels refers to the number of channels (1 for grayscale, 3 for RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5e9aa151",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30298/2187186475.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(X_s1), np.array(X_s2), np.array(y)\n"
     ]
    }
   ],
   "source": [
    "X_train_s1, X_train_s2, y_train = prepare_for_conv_lstm(train_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f41973fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_s1: (2,)\n",
      "Shape of X_train_s2: (2,)\n",
      "Shape of y_train: (2,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of X_train_s1: {X_train_s1.shape}\")\n",
    "print(f\"Shape of X_train_s2: {X_train_s2.shape}\")\n",
    "print(f\"Shape of y_train: {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c010ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-21 18:17:58.858532: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 Branch Shape: (None, 4475520)\n",
      "S2 Branch Shape: (None, 4475520)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " S1_input (InputLayer)          [(None, None, 522,   0           []                               \n",
      "                                544, 1)]                                                          \n",
      "                                                                                                  \n",
      " S2_input (InputLayer)          [(None, None, 522,   0           []                               \n",
      "                                544, 1)]                                                          \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, None, 520, 5  38144       ['S1_input[0][0]']               \n",
      "                                42, 32)                                                           \n",
      "                                                                                                  \n",
      " conv_lstm2d_2 (ConvLSTM2D)     (None, None, 520, 5  38144       ['S2_input[0][0]']               \n",
      "                                42, 32)                                                           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, None, 520, 5  128        ['conv_lstm2d[0][0]']            \n",
      " alization)                     42, 32)                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, None, 520, 5  128        ['conv_lstm2d_2[0][0]']          \n",
      " rmalization)                   42, 32)                                                           \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)     (None, 518, 540, 16  27712       ['batch_normalization[0][0]']    \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv_lstm2d_3 (ConvLSTM2D)     (None, 518, 540, 16  27712       ['batch_normalization_2[0][0]']  \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 518, 540, 16  64         ['conv_lstm2d_1[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 518, 540, 16  64         ['conv_lstm2d_3[0][0]']          \n",
      " rmalization)                   )                                                                 \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 4475520)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 4475520)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 8951040)      0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           572866624   ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 572,998,785\n",
      "Trainable params: 572,998,593\n",
      "Non-trainable params: 192\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, ConvLSTM2D, BatchNormalization, Dense, Flatten, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_shape = (None, 522, 544, 1)\n",
    "\n",
    "s1_input = Input(shape=input_shape, name='S1_input')\n",
    "s2_input = Input(shape=input_shape, name='S2_input')\n",
    "\n",
    "# S1 Branch\n",
    "s1_branch = ConvLSTM2D(32, (3, 3), activation='relu', return_sequences=True)(s1_input)\n",
    "s1_branch = BatchNormalization()(s1_branch)\n",
    "s1_branch = ConvLSTM2D(16, (3, 3), activation='relu', return_sequences=False)(s1_branch)\n",
    "s1_branch = BatchNormalization()(s1_branch)\n",
    "s1_branch = Flatten()(s1_branch)\n",
    "\n",
    "# S2 Branch\n",
    "s2_branch = ConvLSTM2D(32, (3, 3), activation='relu', return_sequences=True)(s2_input)\n",
    "s2_branch = BatchNormalization()(s2_branch)\n",
    "s2_branch = ConvLSTM2D(16, (3, 3), activation='relu', return_sequences=False)(s2_branch)\n",
    "s2_branch = BatchNormalization()(s2_branch)\n",
    "s2_branch = Flatten()(s2_branch)\n",
    "\n",
    "print(\"S1 Branch Shape:\", s1_branch.shape)\n",
    "print(\"S2 Branch Shape:\", s2_branch.shape)\n",
    "\n",
    "# Combine the outputs from both branches\n",
    "combined = concatenate([s1_branch, s2_branch])\n",
    "\n",
    "final_layer = Dense(64, activation='relu')(combined)\n",
    "output = Dense(1, activation='sigmoid')(final_layer)\n",
    "\n",
    "model = Model(inputs=[s1_input, s2_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d6839a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the model with the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ac90aec0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30298/2462548156.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_train_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_s2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/tensorflow/python/framework/constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type numpy.ndarray)."
     ]
    }
   ],
   "source": [
    "model.fit([X_train_s1, X_train_s2], y_train, epochs=10, batch_size=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb3de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ead9596",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_probabilities = model.predict(X_test_s1, X_test_s2)\n",
    "predicted_labels = (predicted_probabilities > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72435389",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Results\n",
    "\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(f\"Test Label: {y_test['i']}, Predicted: {predicted_labels[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
