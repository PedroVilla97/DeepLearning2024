{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de0fb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install -U tifffile[all]\n",
    "# !pip list\n",
    "# Install dependencies\n",
    "!python -m pip install --upgrade pip\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a446796a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 13:21:47.675707: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-04-28 13:21:47.716860: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-28 13:21:47.907148: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-28 13:21:47.908442: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-28 13:21:48.912298: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf \n",
    "from tensorflow.keras import layers, Model\n",
    "from osgeo import gdal\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tiff\n",
    "import os\n",
    "import pandas as pd\n",
    "from skimage.transform import resize\n",
    "from sklearn.model_selection import train_test_split\n",
    "import re\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c4d7120",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_file):\n",
    "    with open(json_file, 'r') as file: \n",
    "        data = json.load(file)\n",
    "    return data \n",
    "\n",
    "data_s1 = load_json_data('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/S1list.json')\n",
    "data_s2 = load_json_data('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/S2list.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed791fcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load images as we saw on kaggle\n",
    "# img = tiff.imread('/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD/0066/S2_2019-02-04_B01.tif')\n",
    "# img_array = np.array(img)\n",
    "# plt.imshow(img)\n",
    "# print(img_array.shape)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4dba727",
   "metadata": {},
   "outputs": [],
   "source": [
    "### This cell is for read the tiff images using the tiff python library\n",
    "### Images, filenames and flooding labels are set to be stored in arrays while reading the images\n",
    "### Function takes a particular directory (A Flloding scenario and fetch for each label in the flooding json files)\n",
    "### It automatically fetch for the number of the folder being processed \n",
    "### And retrieve both the name of the file and the flooding label. e.g from the sequence:\n",
    "### \n",
    "###\n",
    "### {\"0063\": {\"1\": {\"date\": \"2019-02-04\", \"FLOODING\": false, \"FULL-DATA-COVERAGE\": true, \"filename\": \"S2_2019-02-04\"}, \"count\": 1, \"folder\": \"0063\", \"geo\": {\"type\": \"Polygon\", \"coordinates\": [[[28.29722, -15.382762], [28.297507, -15.429039], [28.345216, -15.428755], [28.344918, -15.382479], [28.29722, -15.382762]]]}},\n",
    "### Based on the name of the folder \"0063\" it reads the images of the folder and it appends both filename and flooding label.\n",
    "### To cross and validate whether there is flooding or not the folder \"0200\" is used on the next cell. \n",
    "### This is the second sequence in the S2 json data. Easy to validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136ef27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_from_directory(directory, s1_data, s2_data):\n",
    "    images = []\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    \n",
    "    folder_name = os.path.basename(directory)\n",
    "    print(f\"Processing folder: {folder_name}\")\n",
    "\n",
    "    for file_name in os.listdir(directory):\n",
    "        if file_name.endswith('.tif') or file_name.endswith('.tiff'):\n",
    "            json_data = s1_data if file_name.startswith('S1') else s2_data if file_name.startswith('S2') else None\n",
    "            if json_data is None: \n",
    "                print(f\"Skipping file (not S1 or S2): {file_name}\")\n",
    "                continue\n",
    "            \n",
    "            found = False\n",
    "            for item in json_data.get(folder_name, {}).values():\n",
    "                if isinstance(item, dict) and 'filename' in item and item['filename'] in file_name:\n",
    "                    file_path = os.path.join(directory, file_name)\n",
    "                    img = tiff.imread(file_path)\n",
    "                    img_array = np.array(img)\n",
    "                    images.append(img_array)\n",
    "                    filenames.append(file_name)\n",
    "                    labels.append(item.get('FLOODING', False))\n",
    "                    found = True \n",
    "                    break\n",
    "    return images, filenames, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdbbba48",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Definition to extract the date and order by date so we respect time-series nature for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f50a038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(filename):\n",
    "    # Try to extract S2 date format first\n",
    "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
    "    if match:\n",
    "        return datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
    "    else:\n",
    "        # Try the S1 date format\n",
    "        match = re.search(r'\\d{8}T\\d{6}', filename)\n",
    "        if match:\n",
    "            return datetime.strptime(match.group(), '%Y%m%dT%H%M%S').date()\n",
    "    return None  # Important to handle cases where no date is found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4701e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Following code is to get a list of all the flooding events (folders)\n",
    "### present on the daataset. There is also code to select which folders are\n",
    "### going to be selected to train and test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51fb7136",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### This code is to apply recursively the function to read the images to all \n",
    "### The folders present on the dataset. As the number of the sequence event is stored in a \n",
    "### dictionary everythins is stored on the same dictionary with sequence and flooding label\n",
    "\n",
    "def process_folders(folder_list, main_directory, s1_data, s2_data):\n",
    "    scenario_data = {}\n",
    "    \n",
    "    for folder in folder_list:\n",
    "        directory = os.path.join(main_directory, folder)\n",
    "        images, filenames, labels = load_images_from_directory(directory, s1_data, s2_data)\n",
    "\n",
    "        if not images:\n",
    "            continue\n",
    "        \n",
    "        # Collect data and sort it by date\n",
    "        temp_data = [(img, fname, lbl, extract_date(fname)) for img, fname, lbl in zip(images, filenames, labels) if extract_date(fname) is not None]\n",
    "        temp_data.sort(key=lambda x: x[3]) #Sort by date\n",
    "        \n",
    "        scenario_data[folder] = {\n",
    "            'images': [data[0] for data in temp_data],\n",
    "            'filenames': [data[1] for data in temp_data],\n",
    "            'labels': [data[2] for data in temp_data],\n",
    "            'dates': [data[3] for data in temp_data]\n",
    "        }\n",
    "            \n",
    "    return scenario_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "958d7e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_folders(main_directory):\n",
    "    return [f for f in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, f))]\n",
    "\n",
    "# Main SEN12FLOOD directory, whole data is there\n",
    "main_directory = '/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD'\n",
    "\n",
    "# List all folders\n",
    "all_folders = list_folders(main_directory)\n",
    "\n",
    "# Manually select folders for training and testing\n",
    "# test_folders = [str(i) for i in range(68)]  # Example folders for training\n",
    "# train_folders = ['0001', '0002', '0003', '0004', '0005', '0006', '0007', '0008', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0016', '0018', '0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029', '0030', '0031', '0033', '0034', '0035', '0036', '0037', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0050', '0053', '0054', '0055', '0057', '0059', '0060', '0061', '0063', '0065', '0066', '0067', '0068', '0069', '0070', '0071', '0072', '0073', '0074', '0075', '0076', '0077', '0079', '0080', '0081', '0082', '0084', '0085', '0086', '0088', '0089', '0090', '0091', '0093', '0094', '0095', '0096', '0097', '0098', '0099', '0100', '0101', '0102', '0103', '0104', '0105', '0106', '0107', '0108', '0109', '0111', '0115', '0116', '0117', '0118', '0120', '0121', '0122', '0123', '0124', '0125', '0126', '0127', '0128', '0130', '0131', '0132', '0133', '0134', '0135', '0137', '0138', '0139', '0140', '0141', '0143', '0144', '0145', '0146', '0147', '0148', '0149', '0150', '0151', '0154', '0155', '0156', '0157', '0158', '0159', '0160', '0161', '0162', '0163', '0165', '0166', '0167', '0168', '0169', '0170', '0171', '0173', '0174', '0176', '0177', '0178', '0181', '0182', '0184', '0186', '0187', '0188', '0191', '0192', '0193', '0194', '0196', '0198', '0199', '0200', '0201', '0203', '0204', '0205', '0206', '0207', '0208', '0209', '0210', '0212', '0213', '0214', '0215', '0216', '0217', '0218', '0219', '0220', '0221', '0222', '0223', '0225', '0226', '0227', ' 0229', '0230', '0231', '0232', '0233', '0234', '0235', '0236', '0238', '0240', '0241', '0243', '0244', '0245', '0246', '0247', '0248', '0249', '0250', '0253', '0254', '0255', '0256', '0257', '0258', '0259', '0260', '0261', '0262', '0263', '0266', '0267', '0271', '0272', '0273', '0274', '0275', '0276', '0277', '0278', '0279', '0280', '0281', '0282', '0285', '0286', '0287', '0288', '0290', '0293', '0294', '0295', '0296', '0298', '0299', '0300', '0301', '0303', ' 0304', '0305', '0306', '0307', '0308', '0309', '0310', '0311', '0313', '0316', '0318', '0319', '0320', '0321', '0323', '0324', '0325', '0326', '0327', '0328', '0329', '0330', '0331', '0332', '0333', '0334', '0335', '0336']   # Example folders for testing\n",
    "train_folders = ['0018']\n",
    "test_folders = ['24']\n",
    "validation_folders = ['30']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7c77b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEPNG THE CODE BELOW. DISCUSS IT WITH PEDRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f3436db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Preprocess train and test folders\n",
    "# train_images, train_labels, train_scenario_labels = process_folders(train_folders, main_directory, data_s1, data_s2)\n",
    "# test_images, test_labels, test_scenario_labels = process_folders(test_folders, main_directory, data_s1, data_s2)\n",
    "# validation_images, validation_labels, validation_scenario_labels = process_folders(validation_folders, main_directory, data_s1, data_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa4e50e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing folder: 0018\n",
      "Processing folder: 24\n",
      "Processing folder: 30\n"
     ]
    }
   ],
   "source": [
    "### Preprocess train and test folders\n",
    "train_data = process_folders(train_folders, main_directory, data_s1, data_s2)\n",
    "test_data = process_folders(test_folders, main_directory, data_s1, data_s2)\n",
    "validation_data = process_folders(validation_folders, main_directory, data_s1, data_s2)\n",
    "\n",
    "# Example of accessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8e3b1806",
   "metadata": {},
   "outputs": [],
   "source": [
    "### NP.array and resize images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d08c709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.transform import resize\n",
    "\n",
    "def resize_and_convert_images(grouped_data, target_shape):\n",
    "    for scenario, data in grouped_data.items():\n",
    "        # Resize images and explicitly convert to NumPy arrays if not already\n",
    "        data['images'] = [np.array(resize(img, target_shape, preserve_range=True), dtype=np.float32) for img in data['images']]\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fe36967",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_shape = (522, 544)  # Define the target shape for resizing images\n",
    "train_data_resized = resize_and_convert_images(train_data, target_shape)\n",
    "test_data_resized = resize_and_convert_images(test_data, target_shape)\n",
    "validation_data_resized = resize_and_convert_images(validation_data, target_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536bf882",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7dd5cbd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Structure the data to prepare the input for the ConvLSTM neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "af296a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from collections import defaultdict \n",
    "\n",
    "def group_by_scenario_and_type(scenario_data):\n",
    "    grouped_data = defaultdict(lambda: {'S1': [], 'S2': [], 'labels': []})\n",
    "    for scenario, data in scenario_data.items():\n",
    "        for img, lbl, fname in zip(data['images'], data['labels'], data['filenames']):\n",
    "            if 'S1' in fname:\n",
    "                grouped_data[scenario]['S1'].append(img)\n",
    "            elif 'S2' in fname:\n",
    "                grouped_data[scenario]['S2'].append(img)\n",
    "            grouped_data[scenario]['labels'].append(lbl)  # Assuming labels are tied to images\n",
    "    return grouped_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "67ef7200",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_grouped = group_by_scenario_and_type(train_data_resized)\n",
    "test_grouped = group_by_scenario_and_type(test_data_resized)\n",
    "validation_grouped = group_by_scenario_and_type(validation_data_resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fcac384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "243f2d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(scenario_data, target_shape):\n",
    "    X_s1, X_s2, y = [], [], []\n",
    "\n",
    "    for scenario, data in scenario_data.items():\n",
    "        # Since images might already be resized, check if resizing is needed again\n",
    "        images_s1 = [resize(img, target_shape, preserve_range=True, anti_aliasing=True) if img.shape[:2] != target_shape else img for img in data['S1']]\n",
    "        images_s2 = [resize(img, target_shape, preserve_range=True, anti_aliasing=True) if img.shape[:2] != target_shape else img for img in data['S2']]\n",
    "        labels = data['labels']\n",
    "\n",
    "        X_s1.append(np.array(images_s1))\n",
    "        X_s2.append(np.array(images_s2))\n",
    "        y.append(np.array(labels))\n",
    "\n",
    "    # Concatenate arrays from each scenario to form a continuous array\n",
    "    X_s1 = np.concatenate(X_s1, axis=0) if X_s1 else np.array([], dtype=np.float32)\n",
    "    X_s2 = np.concatenate(X_s2, axis=0) if X_s2 else np.array([], dtype=np.float32)\n",
    "    y = np.concatenate(y, axis=0) if y else np.array([], dtype=np.float32)\n",
    "\n",
    "    return X_s1, X_s2, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "045061d6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#### Erase the previous model. Strategy must be to train, test and validate model \n",
    "#### By sequences and not by all te images not considering the scenario (folder) label\n",
    "\n",
    "#### After consultating with my doctor he has recommend me to go with the \n",
    "### Convolutional LSTM as it is convenient for time-series works\n",
    "### And also can work multidimensionally which make it a good fit for image and video analysis\n",
    "\n",
    "#### Define the model\n",
    "\n",
    "# Structure Data Appropriately: Each input for the ConvLSTM needs to be shaped as (samples, time_steps, height, width, channels), where samples is the number of sequences, time_steps is the number of images in each sequence, height and width are the dimensions of each image, and channels refers to the number of channels (1 for grayscale, 3 for RGB)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5e9aa151",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training shapes: (16, 522, 544) (108, 522, 544) (124,)\n",
      "Testing shapes: (6, 522, 544) (36, 522, 544) (42,)\n",
      "Validation shapes: (16, 522, 544) (108, 522, 544) (124,)\n"
     ]
    }
   ],
   "source": [
    "X_train_s1, X_train_s2, y_train = prepare_data_for_model(train_grouped, target_shape=(522, 544))\n",
    "X_test_s1, X_test_s2, y_test = prepare_data_for_model(test_grouped, target_shape=(522, 544))\n",
    "X_val_s1, X_val_s2, y_val = prepare_data_for_model(validation_grouped, target_shape=(522, 544))\n",
    "\n",
    "# Check the shape and ensure they match expected dimensions\n",
    "print(\"Training shapes:\", X_train_s1.shape, X_train_s2.shape, y_train.shape)\n",
    "print(\"Testing shapes:\", X_test_s1.shape, X_test_s2.shape, y_test.shape)\n",
    "print(\"Validation shapes:\", X_val_s1.shape, X_val_s2.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eedec771",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03c1c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Here we need to introduce \"padding\" to work in the different lenghts of the \n",
    "### sequences as different sequences have a different number of images.\n",
    "### paddding is the way of fixing this dimensionality problem\n",
    "\n",
    "#Masking: After padding, using a Masking layer tells the model to ignore the padding during training, which helps in focusing on the meaningful data. Without masking, the model might learn the padding as a significant part of the input pattern, potentially skewing results.\n",
    "### Padding at the beggining is the common practice when dealing with time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0446bbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_s1_padded: (16, 522, 544)\n",
      "Shape of X_train_s2_padded: (108, 522, 544)\n",
      "Shape of y_train: (124,)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Input, Masking, ConvLSTM2D, BatchNormalization, Dense, Flatten, concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# def pad_sequences_uniformly(X_s1, X_s2, max_len=None):\n",
    "    \n",
    "#     if not max_len: \n",
    "#         max_len = max(max(seq.shape[0] for seq in X_s1), max(seq.shape[0] for seq in X_s2))\n",
    "#     X_s1_padded = [pad_sequences([seq], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0] for seq in X_s1]\n",
    "#     X_s2_padded = [pad_sequences([seq], maxlen=max_len, dtype='float32', padding='post', truncating='post', value=0.0)[0] for seq in X_s2]\n",
    "    \n",
    "#     return np.array(X_s1_padded), np.array(X_s2_padded)\n",
    "\n",
    "def pad_image_sequences(sequences, maxlen=None, dtype='float32'):\n",
    "    # Pad each sequence to the same length\n",
    "    padded_sequences = pad_sequences(sequences, maxlen=maxlen, dtype=dtype, padding='post', truncating='post', value=0.0)\n",
    "    return padded_sequences\n",
    "\n",
    "# Determine the maximum sequence length across both S1 and S2\n",
    "max_len = max([len(seq) for seq in X_train_s1] + [len(seq) for seq in X_train_s2])\n",
    "\n",
    "# Pad S1 and S2 training data\n",
    "X_train_s1_padded = pad_image_sequences(X_train_s1, maxlen=max_len)\n",
    "X_train_s2_padded = pad_image_sequences(X_train_s2, maxlen=max_len)\n",
    "\n",
    "# Ensure dimensions are correct\n",
    "print(\"Shape of X_train_s1_padded:\", X_train_s1_padded.shape)\n",
    "print(\"Shape of X_train_s2_padded:\", X_train_s2_padded.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a2af93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d5b6d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c010ef10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-28 13:22:03.530741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:266] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 Branch Shape: (None, 2237760)\n",
      "S2 Branch Shape: (None, 2237760)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " S1_input (InputLayer)          [(None, None, 522,   0           []                               \n",
      "                                544, 1)]                                                          \n",
      "                                                                                                  \n",
      " S2_input (InputLayer)          [(None, None, 522,   0           []                               \n",
      "                                544, 1)]                                                          \n",
      "                                                                                                  \n",
      " conv_lstm2d (ConvLSTM2D)       (None, None, 520, 5  9856        ['S1_input[0][0]']               \n",
      "                                42, 16)                                                           \n",
      "                                                                                                  \n",
      " conv_lstm2d_2 (ConvLSTM2D)     (None, None, 520, 5  9856        ['S2_input[0][0]']               \n",
      "                                42, 16)                                                           \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, None, 520, 5  64         ['conv_lstm2d[0][0]']            \n",
      " alization)                     42, 16)                                                           \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, None, 520, 5  64         ['conv_lstm2d_2[0][0]']          \n",
      " rmalization)                   42, 16)                                                           \n",
      "                                                                                                  \n",
      " conv_lstm2d_1 (ConvLSTM2D)     (None, 518, 540, 8)  6944        ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv_lstm2d_3 (ConvLSTM2D)     (None, 518, 540, 8)  6944        ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 518, 540, 8)  32         ['conv_lstm2d_1[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 518, 540, 8)  32         ['conv_lstm2d_3[0][0]']          \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2237760)      0           ['batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 2237760)      0           ['batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 4475520)      0           ['flatten[0][0]',                \n",
      "                                                                  'flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 64)           286433344   ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (None, 1)            65          ['dense[0][0]']                  \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 286,467,201\n",
      "Trainable params: 286,467,105\n",
      "Non-trainable params: 96\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (None, 522, 544, 1)\n",
    "\n",
    "s1_input = Input(shape=input_shape, name='S1_input')\n",
    "s2_input = Input(shape=input_shape, name='S2_input')\n",
    "\n",
    "# S1 Branch\n",
    "s1_branch = ConvLSTM2D(16, (3, 3), activation='relu', return_sequences=True)(s1_input)\n",
    "s1_branch = BatchNormalization()(s1_branch)\n",
    "s1_branch = ConvLSTM2D(8, (3, 3), activation='relu', return_sequences=False)(s1_branch)\n",
    "s1_branch = BatchNormalization()(s1_branch)\n",
    "s1_branch = Flatten()(s1_branch)\n",
    "\n",
    "# S2 Branch\n",
    "s2_branch = ConvLSTM2D(16, (3, 3), activation='relu', return_sequences=True)(s2_input)\n",
    "s2_branch = BatchNormalization()(s2_branch)\n",
    "s2_branch = ConvLSTM2D(8, (3, 3), activation='relu', return_sequences=False)(s2_branch)\n",
    "s2_branch = BatchNormalization()(s2_branch)\n",
    "s2_branch = Flatten()(s2_branch)\n",
    "\n",
    "print(\"S1 Branch Shape:\", s1_branch.shape)\n",
    "print(\"S2 Branch Shape:\", s2_branch.shape)\n",
    "\n",
    "# Combine the outputs from both branches\n",
    "combined = concatenate([s1_branch, s2_branch])\n",
    "\n",
    "final_layer = Dense(64, activation='relu')(combined)\n",
    "output = Dense(1, activation='sigmoid')(final_layer)\n",
    "\n",
    "model = Model(inputs=[s1_input, s2_input], outputs=output)\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5633f3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_required_dimensions(X):\n",
    "    # Add a time dimension if not present, and ensure there's a channel dimension\n",
    "    # This is assuming X is a list of 3D arrays [samples, rows, cols]\n",
    "    # We'll treat the entire set as a single batch with multiple time steps here for demonstration\n",
    "    if X.ndim == 3:  # [samples, rows, cols]\n",
    "        X = X[:, np.newaxis, :, :, np.newaxis]  # Reshape to [samples, 1 (time), rows, cols, 1 (channel)]\n",
    "    elif X.ndim == 4:  # [samples, time, rows, cols]\n",
    "        X = X[:, :, :, :, np.newaxis]  # Add channel dimension\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b351f53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Shape of X_train_s1_padded: (16, 1, 522, 544, 1)\n",
      "Adjusted Shape of X_train_s2_padded: (108, 1, 522, 544, 1)\n"
     ]
    }
   ],
   "source": [
    "# Apply the function\n",
    "X_train_s1_padded = add_required_dimensions(X_train_s1_padded)\n",
    "X_train_s2_padded = add_required_dimensions(X_train_s2_padded)\n",
    "\n",
    "print(\"Adjusted Shape of X_train_s1_padded:\", X_train_s1_padded.shape)\n",
    "print(\"Adjusted Shape of X_train_s2_padded:\", X_train_s2_padded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "69c55468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d6839a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Fit the model with the training sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ac90aec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "12/12 [==============================] - 42s 3s/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 584.6135 - val_accuracy: 0.0000e+00\n",
      "Epoch 2/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 316.6407 - val_accuracy: 0.0000e+00\n",
      "Epoch 3/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 219.8088 - val_accuracy: 0.0000e+00\n",
      "Epoch 4/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 144.2776 - val_accuracy: 0.2500\n",
      "Epoch 5/10\n",
      "12/12 [==============================] - 39s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 101.0199 - val_accuracy: 0.5000\n",
      "Epoch 6/10\n",
      "12/12 [==============================] - 39s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 69.6228 - val_accuracy: 0.5000\n",
      "Epoch 7/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 44.2739 - val_accuracy: 0.5000\n",
      "Epoch 8/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 23.1224 - val_accuracy: 0.5000\n",
      "Epoch 9/10\n",
      "12/12 [==============================] - 41s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 9.4985 - val_accuracy: 0.7500\n",
      "Epoch 10/10\n",
      "12/12 [==============================] - 40s 3s/step - loss: 0.0000e+00 - accuracy: 1.0000 - val_loss: 1.9199 - val_accuracy: 0.7500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x778fe7ffbe50>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([X_train_s1_padded, X_train_s2_padded], y_train, epochs=10, batch_size=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c758fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary dimensions for the model to process the inputs correctly, similar to the training data preparation\n",
    "X_test_s1 = add_required_dimensions(X_test_s1)\n",
    "X_test_s2 = add_required_dimensions(X_test_s2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dbb3de09",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5ead9596",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Data cardinality is ambiguous:\n  x sizes: 6, 36\nMake sure all arrays contain the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_43513/1788596298.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Assuming X_test_s1 and X_test_s2 are your test sets for the two types of inputs and are properly formatted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredicted_probabilities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_test_s1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_s2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mpredicted_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpredicted_probabilities\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m_check_data_cardinality\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1850\u001b[0m             )\n\u001b[1;32m   1851\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Make sure all arrays contain the same number of samples.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1852\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1854\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 6, 36\nMake sure all arrays contain the same number of samples."
     ]
    }
   ],
   "source": [
    "# Assuming X_test_s1 and X_test_s2 are your test sets for the two types of inputs and are properly formatted\n",
    "predicted_probabilities = model.predict([X_test_s1, X_test_s2])\n",
    "predicted_labels = (predicted_probabilities > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72435389",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Print Results\n",
    "\n",
    "for i in range(len(predicted_labels)):\n",
    "    print(f\"Test Label: {y_test['i']}, Predicted: {predicted_labels[i]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
