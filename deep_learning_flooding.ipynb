{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bed73d1e-8b11-4e11-8e46-dc7099d9a860",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Connect to Azure Workspace and Create Environment\n",
        "\n",
        "Connect to your Azure workspace using a configuration file\n",
        "and create an environment from a Conda specification file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8320a57f-cc34-400a-aadd-348286d8c221",
      "metadata": {
        "gather": {
          "logged": 1715060168935
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core import Workspace, Environment\n",
        "\n",
        "ws = Workspace.from_config()\n",
        "\n",
        "env = Environment.from_conda_specification(\n",
        "    name='myenv',\n",
        "    file_path='environment.yaml'\n",
        ")\n",
        "\n",
        "env.register(workspace=ws)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f6688f2-24f8-4c86-8abe-c706ae598523",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Set Up Compute Target\n",
        "\n",
        "Set up a compute target in Azure using an existing compute cluster.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbc72f76-c8c6-40e9-b761-071fd3a6aa1d",
      "metadata": {
        "gather": {
          "logged": 1715060169410
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from azureml.core.compute import ComputeTarget, AmlCompute\n",
        "from azureml.core.compute_target import ComputeTargetException\n",
        "\n",
        "compute_target = ComputeTarget(workspace=ws, name=\"pedrovillabe169\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85baed67-a8f7-46c2-8397-df349752ddfa",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Import Required Libraries\n",
        "\n",
        "Import necessary libraries including TensorFlow, NumPy, \n",
        "and various utilities for processing images and data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48842211-f3f3-4d5f-85f2-950a1b68b462",
      "metadata": {
        "gather": {
          "logged": 1715060210760
        }
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf \n",
        "from tensorflow.keras import layers, Model\n",
        "import numpy as np\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import tifffile as tiff\n",
        "import os\n",
        "import pandas as pd\n",
        "from skimage.transform import resize\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from datetime import datetime\n",
        "from os.path import join\n",
        "from collections import defaultdict\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Masking, ConvLSTM2D, BatchNormalization, Dense, Flatten, concatenate, GlobalAveragePooling2D, Embedding, TimeDistributed, GlobalAveragePooling3D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35b4f0d0-49ec-49e4-82ca-764e097ccfd9",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load Data from JSON Files\n",
        "\n",
        "Load Sentinel-1 and Sentinel-2 metadata from JSON files \n",
        "containing information about the satellite images.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4d7120",
      "metadata": {
        "gather": {
          "logged": 1715028921823
        }
      },
      "outputs": [],
      "source": [
        "def load_json_data(json_file):\n",
        "    with open(json_file, 'r') as file: \n",
        "        data = json.load(file)\n",
        "    return data \n",
        "\n",
        "azure_base_path = 'SEN12FLOOD'\n",
        "\n",
        "data_s1 = load_json_data(join(azure_base_path, 'S1list.json'))\n",
        "data_s2 = load_json_data(join(azure_base_path, 'S2list.json'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b90cbfc3-11d1-45d4-b695-67639694b8de",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Define Helper Functions\n",
        "\n",
        "Define helper functions for image processing, including combining \n",
        "Sentinel-1 polarizations and Sentinel-2 bands, and for extracting dates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e41f79fc-3722-4af6-b096-10701fb2a336",
      "metadata": {
        "gather": {
          "logged": 1715028923290
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "def use_single_channel_as_grayscale(vv, vh, use_vv=True):\n",
        "\n",
        "    return vv if use_vv else vh\n",
        "\n",
        "def combine_s1_polarizations(images):\n",
        "\n",
        "    return [img['VV'] if 'VV' in img else np.zeros((522, 544)) for img in images]\n",
        "\n",
        "def combine_s2_bands(images_dicts, desired_bands, base_resolution=10):\n",
        "    combined_images = []\n",
        "    for images in images_dicts:\n",
        "        band_images = []\n",
        "        for band in desired_bands:\n",
        "            band_key = f\"B{band}\"\n",
        "            if band_key in images:\n",
        "                band_image = images[band_key]\n",
        "                band_res = band_resolution(band_key)\n",
        "                resolution_factor = base_resolution / band_res\n",
        "                if resolution_factor != 1:\n",
        "                    band_image = resize(band_image, (int(band_image.shape[0] * resolution_factor),\n",
        "                                                     int(band_image.shape[1] * resolution_factor)),\n",
        "                                        preserve_range=True, anti_aliasing=True)\n",
        "                band_images.append(band_image)\n",
        "            else:\n",
        "                band_res = band_resolution(band_key)\n",
        "                resolution_factor = base_resolution / band_res\n",
        "                band_images.append(np.zeros((int(522 * resolution_factor), int(544 * resolution_factor))))\n",
        "        \n",
        "        shape = band_images[0].shape\n",
        "        for i, band_image in enumerate(band_images):\n",
        "            if band_image.shape != shape:\n",
        "                band_images[i] = resize(band_image, shape, preserve_range=True, anti_aliasing=True)\n",
        "\n",
        "        combined_image = np.stack(band_images, axis=-1)\n",
        "        combined_images.append(combined_image)\n",
        "    return combined_images\n",
        "\n",
        "def band_resolution(band_key):\n",
        "\n",
        "    band_resolutions = {\n",
        "        \"B1\": 60, \"B2\": 10, \"B3\": 10, \"B4\": 10,\n",
        "        \"B5\": 20, \"B6\": 20, \"B7\": 20, \"B8\": 10,\n",
        "        \"B8A\": 20, \"B9\": 60, \"B11\": 20, \"B12\": 20\n",
        "    }\n",
        "    return band_resolutions.get(band_key, 10)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a028ea4d-b0c5-4bf6-8749-fe1bb6251621",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Load Images from Directory\n",
        "\n",
        "Define a function to load images from a specified directory.\n",
        "The function reads Sentinel-1 and Sentinel-2 images from TIFF files\n",
        "and organizes them by their corresponding metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "136ef27e",
      "metadata": {
        "gather": {
          "logged": 1715028924747
        }
      },
      "outputs": [],
      "source": [
        "def load_images_from_directory(directory, s1_data, s2_data, use_vv=True, desired_bands=[2, 3, 4]):\n",
        "    images = []\n",
        "    filenames = []\n",
        "    labels = []\n",
        "\n",
        "    folder_name = os.path.basename(directory)\n",
        "    print(f\"Processing folder: {folder_name}\")\n",
        "\n",
        "    for file_name in os.listdir(directory):\n",
        "        if file_name.endswith('.tif') or file_name.endswith('.tiff'):\n",
        "            if file_name.startswith('S1') and file_name.endswith('_VH.tif'):\n",
        "                json_data = s1_data\n",
        "                layer_name = \"VH\"\n",
        "            elif file_name.startswith('S2'):\n",
        "                json_data = s2_data\n",
        "                layer_name = file_name.split('_')[-1].replace('.tif', '')  # Get the last part as the layer\n",
        "            else:\n",
        "                print(f\"Skipping file (not S1 or S2 or not ending with _VH.tif): {file_name}\")\n",
        "                continue\n",
        "\n",
        "            base_filename = '_'.join(file_name.split('_')[:-1])\n",
        "\n",
        "            found = False\n",
        "            for item in json_data.get(folder_name, {}).values():\n",
        "                if isinstance(item, dict) and 'filename' in item and item['filename'] in base_filename:\n",
        "                    file_path = os.path.join(directory, file_name)\n",
        "                    img = tiff.imread(file_path)\n",
        "                    img_array = np.array(img)\n",
        "\n",
        "                    if base_filename not in filenames:\n",
        "                        filenames.append(base_filename)\n",
        "                        images.append({})\n",
        "                        labels.append(item.get('FLOODING', False))\n",
        "\n",
        "                    index = filenames.index(base_filename)\n",
        "                    images[index][layer_name] = img_array\n",
        "                    found = True\n",
        "                    break\n",
        "\n",
        "            if not found:\n",
        "                print(f\"Skipping file (metadata not found): {file_name}\")\n",
        "\n",
        "    for idx, img_dict in enumerate(images):\n",
        "        if any(key.startswith(\"B\") for key in img_dict):  \n",
        "            img_dict['rgb'] = combine_s2_bands([img_dict], desired_bands)[0]\n",
        "\n",
        "    return images, filenames, labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7135e0ee-d0ae-4565-8aab-33029cec43c5",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Extract Date from Filename\n",
        "\n",
        "Define a function to extract dates from filenames.\n",
        "This is used to sort images chronologically for each scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f50a038",
      "metadata": {
        "gather": {
          "logged": 1715028925925
        }
      },
      "outputs": [],
      "source": [
        "def extract_date(filename):\n",
        "    match = re.search(r'\\d{4}-\\d{2}-\\d{2}', filename)\n",
        "    if match:\n",
        "        return datetime.strptime(match.group(), '%Y-%m-%d').date()\n",
        "    else:\n",
        "        match = re.search(r'\\d{8}T\\d{6}', filename)\n",
        "        if match:\n",
        "            return datetime.strptime(match.group(), '%Y%m%dT%H%M%S').date()\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ded05e27-e554-44d2-a5f0-886b896a595c",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Process Folders\n",
        "\n",
        "Define a function to process all folders and load the corresponding images.\n",
        "This function reads and organizes images based on the flood scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fb7136",
      "metadata": {
        "gather": {
          "logged": 1715028926904
        }
      },
      "outputs": [],
      "source": [
        "def process_folders(folder_list, main_directory, s1_data, s2_data):\n",
        "    scenario_data = {}\n",
        "    \n",
        "    for folder in folder_list:\n",
        "        directory = os.path.join(main_directory, folder)\n",
        "        images, filenames, labels = load_images_from_directory(directory, s1_data, s2_data)\n",
        "\n",
        "        print(f\"Folder: {folder}, Files Loaded: {len(filenames)}\")  \n",
        "\n",
        "        if not images:\n",
        "            continue\n",
        "        \n",
        "        temp_data = []\n",
        "        for img, fname, lbl in zip(images, filenames, labels):\n",
        "            date = extract_date(fname)\n",
        "            if date is not None:\n",
        "                temp_data.append((img, fname, lbl, date))\n",
        "                print(f\"Filename: {fname}, Date: {date}, Label: {lbl}\")  \n",
        "\n",
        "        temp_data.sort(key=lambda x: x[3]) # ORGANIZE THE DATA BY DATE. VEY IMPORTANT FOR RESPECT TIME-SERIES\n",
        "        \n",
        "        scenario_data[folder] = {\n",
        "            'images': [data[0] for data in temp_data],\n",
        "            'filenames': [data[1] for data in temp_data],\n",
        "            'labels': [data[2] for data in temp_data],\n",
        "            'dates': [data[3] for data in temp_data]\n",
        "        }\n",
        "            \n",
        "    return scenario_data\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52fc6b45-1b20-42c8-b495-0b900d6a20b5",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "List of all the readable scenarios. About 50 scenarios were not able to proccess due to a problem with the tiff library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "958d7e0a",
      "metadata": {
        "gather": {
          "logged": 1715028927955
        }
      },
      "outputs": [],
      "source": [
        "def list_folders(main_directory):\n",
        "    return [f for f in os.listdir(main_directory) if os.path.isdir(os.path.join(main_directory, f))]\n",
        "\n",
        "# Main SEN12FLOOD directory, whole data is there\n",
        "# main_directory = '/home/pedro/Documents/JADS/DeepLearning/SEN12FLOOD'\n",
        "main_directory = azure_base_path\n",
        "# List all folders\n",
        "# all_folders = list_folders(main_directory)\n",
        "\n",
        "# Manually select folders for training and testing\n",
        "a_folders = [str(i) for i in range(68)]  # Example folders for training\n",
        "b_folders = ['0001', '0004', '0005', '0006', '0007', '0009', '0010', '0011', '0012', '0013', '0014', '0015', '0018', '0020', '0021', '0022', '0023', '0024', '0025', '0026', '0027', '0028', '0029', '0030', '0031', '0033', '0034', '0035', '0036', '0037', '0042', '0043', '0044', '0045', '0046', '0047', '0048', '0050', '0053', '0054', '0055', '0057', '0059', '0060', '0061', '0063', '0065', '0066', '0067', '0068', '0069', '0070', '0071', '0072', '0073', '0074', '0081', '0082', '0085', '0093', '0094', '0095', '0096', '0098', '0099', '0111', '0115', '0125', '0126', '0128', '0130', '0131', '0132', '0133', '0134', '0135', '0137', '0138', '0139', '0140', '0143', '0144', '0145', '0146', '0147', '0148', '0149', '0150', '0151', '0155', '0156', '0157', '0158', '0159', '0160', '0161', '0162', '0163', '0166', '0167', '0168', '0169', '0170', '0171', '0173', '0174', '0176', '0177', '0178', '0181', '0182', '0184', '0186', '0187', '0188', '0191', '0192', '0193', '0194', '0196', '0198', '0199', '0200', '0201', '0204', '0205', '0206', '0207', '0208', '0209', '0210', '0212', '0213', '0214', '0215', '0216', '0217', '0218', '0219', '0220', '0221', '0222', '0223', '0225', '0226', '0227', '0230', '0231', '0232', '0233', '0234', '0235', '0236', '0238', '0240', '0241', '0243', '0244', '0245', '0246', '0247', '0248', '0249', '0250', '0253', '0254', '0255', '0256', '0257', '0258', '0259', '0260', '0261', '0262', '0263', '0266', '0267', '0271', '0272', '0273', '0274', '0275', '0276', '0277', '0278', '0279', '0280', '0281', '0282', '0285', '0286', '0287', '0288', '0290', '0293', '0294', '0295', '0296', '0298', '0299', '0300', '0301', '0303', '0305', '0306', '0307', '0308', '0309', '0310', '0311', '0313', '0316', '0318', '0319', '0320', '0321', '0323', '0324', '0325', '0326', '0327', '0328', '0329', '0330', '0331', '0332', '0333', '0334', '0335', '0336']   # Example folders for testing\n",
        "\n",
        "all_folders = a_folders + b_folders"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "31de7c72-1dda-4b95-b2c4-837a3d06e8c4",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get Flooding and Non-Flooding Scenarios\n",
        "\n",
        "Define a function to identify scenarios that contain flooding\n",
        "and those that do not. This helps in balancing the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b4da6f-17a1-4ff9-9984-addb6346e415",
      "metadata": {
        "gather": {
          "logged": 1715029752192
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "def get_flooding_and_non_flooding_scenarios(folder_list, main_directory, s1_data, s2_data):\n",
        "    flooding_scenarios = []\n",
        "    non_flooding_scenarios = []\n",
        "\n",
        "    for folder in folder_list:\n",
        "        scenario_data = process_folders([folder], main_directory, s1_data, s2_data)\n",
        "        folder_scenario = scenario_data.get(folder)\n",
        "\n",
        "        if folder_scenario:\n",
        "            labels = folder_scenario['labels']\n",
        "            if any(labels):  \n",
        "                flooding_scenarios.append(folder)\n",
        "            else:  \n",
        "                non_flooding_scenarios.append(folder)\n",
        "    \n",
        "    return flooding_scenarios, non_flooding_scenarios\n",
        "\n",
        "flooding_scenarios, non_flooding_scenarios = get_flooding_and_non_flooding_scenarios(all_folders, main_directory, data_s1, data_s2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b8a1a93-77c0-43c7-941e-34951aad85c4",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Select Balanced Subset of Scenarios\n",
        "\n",
        "Define a function to select a balanced subset of flooding and non-flooding scenarios.\n",
        "This is used to ensure an even distribution for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b3343176-ce17-4396-b77e-9b8bbf9715f9",
      "metadata": {
        "gather": {
          "logged": 1715037597600
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "def select_balanced_subset(flooding_scenarios, non_flooding_scenarios, fraction=1):\n",
        "    min_count = min(len(flooding_scenarios), len(non_flooding_scenarios))\n",
        "    subset_size = int(min_count * fraction)\n",
        "\n",
        "    flooding_subset = random.sample(flooding_scenarios, subset_size)\n",
        "    non_flooding_subset = random.sample(non_flooding_scenarios, subset_size)\n",
        "\n",
        "    return flooding_subset + non_flooding_subset\n",
        "\n",
        "balanced_subset = select_balanced_subset(flooding_scenarios, non_flooding_scenarios, fraction=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37458e32-08d6-4a7b-bcff-a284e37326ee",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Split Scenarios into Train, Validation, and Test Sets\n",
        "\n",
        "Define a function to split the scenarios into training, validation, and testing sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2241379-9e71-4ed9-892f-31b9aaab5c4d",
      "metadata": {
        "gather": {
          "logged": 1715037598828
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def split_scenarios(scenarios, train_ratio=0.6, val_ratio=0.2):\n",
        "    random.shuffle(scenarios)\n",
        "    total_size = len(scenarios)\n",
        "    train_size = int(train_ratio * total_size)\n",
        "    val_size = int(val_ratio * total_size)\n",
        "\n",
        "    train_scenarios = scenarios[:train_size]\n",
        "    val_scenarios = scenarios[train_size:train_size + val_size]\n",
        "    test_scenarios = scenarios[train_size + val_size:]\n",
        "\n",
        "    return train_scenarios, val_scenarios, test_scenarios\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7c77b37",
      "metadata": {
        "gather": {
          "logged": 1715037599323
        }
      },
      "outputs": [],
      "source": [
        "train_folders, validation_folders, test_folders = split_scenarios(balanced_subset)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9878af7f-b5ec-4d86-9468-9bb88234e1dd",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Process Folders for Train, Validation, and Test Sets\n",
        "\n",
        "Process the folders for each set and store the images and metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4e50e2",
      "metadata": {
        "gather": {
          "logged": 1715035535822
        }
      },
      "outputs": [],
      "source": [
        "train_data = process_folders(train_folders, main_directory, data_s1, data_s2)\n",
        "val_data = process_folders(validation_folders, main_directory, data_s1, data_s2)\n",
        "test_data = process_folders(test_folders, main_directory, data_s1, data_s2)\n",
        "\n",
        "print(f\"Training folders: {train_folders}\")\n",
        "print(f\"Validation folders: {validation_folders}\")\n",
        "print(f\"Testing folders: {test_folders}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7ea8423-e320-4313-ae8d-a734ba0ff621",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Resize and Convert Images\n",
        "\n",
        "Define a function to resize and convert images to the target shape.\n",
        "This ensures that all images have a consistent shape for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d08c709a",
      "metadata": {
        "gather": {
          "logged": 1715035567706
        }
      },
      "outputs": [],
      "source": [
        "def resize_and_convert_images(img_dict, target_shape):\n",
        "    resized_img_dict = {}\n",
        "    for key, img in img_dict.items():\n",
        "        if isinstance(img, np.ndarray):\n",
        "            if img.ndim == 4 and img.shape[0] == 1:  # (1, H, W, C)\n",
        "                img = img[0]  \n",
        "\n",
        "            if img.ndim == 3 and img.shape[2] == 2:  # (H, W, 2)\n",
        "                img = img[:, :, 0]  \n",
        "\n",
        "            if img.ndim == 2:  \n",
        "                img_resized = resize(img, target_shape[:2], preserve_range=True, anti_aliasing=True)\n",
        "                img_resized = img_resized[:, :, np.newaxis]  \n",
        "            elif img.ndim == 3 and img.shape[2] <= target_shape[2]:  \n",
        "                img_resized = resize(img, (target_shape[0], target_shape[1], img.shape[2]), preserve_range=True, anti_aliasing=True)\n",
        "            else:\n",
        "                raise ValueError(f\"Unexpected image shape: {img.shape}\")\n",
        "\n",
        "            resized_img_dict[key] = np.array(img_resized, dtype=np.float32)\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected type for image data: {type(img)}\")\n",
        "    \n",
        "    return resized_img_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d0396cd-029d-4d5f-8707-729153d29c16",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Group Data by Scenario and Type\n",
        "\n",
        "Define a function to group the data by scenario and type.\n",
        "This organizes the data into a structure suitable for training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dd5cbd8",
      "metadata": {
        "gather": {
          "logged": 1715035569242
        }
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "from collections import defaultdict \n",
        "\n",
        "\n",
        "def group_by_scenario_and_type(scenario_data, target_shape_s1, target_shape_s2):\n",
        "    grouped_data = defaultdict(lambda: {'S1': [], 'S2': [], 'S1_filenames': [], 'S2_filenames': [], 'labels': [], 'dates': []})\n",
        "    \n",
        "    for scenario, data in scenario_data.items():\n",
        "        for img_dict, lbl, fname, date in zip(data['images'], data['labels'], data['filenames'], data['dates']):\n",
        "            if 'S1' in fname:\n",
        "                resized_img_dict = resize_and_convert_images(img_dict, target_shape_s1)\n",
        "                grouped_data[scenario]['S1'].append(resized_img_dict)\n",
        "                grouped_data[scenario]['S1_filenames'].append(fname)\n",
        "            elif 'S2' in fname:\n",
        "                resized_img_dict = resize_and_convert_images(img_dict, target_shape_s2)\n",
        "                grouped_data[scenario]['S2'].append(resized_img_dict)\n",
        "                grouped_data[scenario]['S2_filenames'].append(fname)\n",
        "            grouped_data[scenario]['labels'].append(lbl)\n",
        "            grouped_data[scenario]['dates'].append(date)\n",
        "            print(f\"Scenario: {scenario}, File: {fname}, Type: {'S1' if 'S1' in fname else 'S2'}, Date: {date}, Label: {lbl}\")\n",
        "    \n",
        "    return grouped_data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "462d5e2f-5470-4772-a20f-68857661cdb8",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Group Training, Validation, and Testing Data\n",
        "\n",
        "Group the training, validation, and testing data by scenario and type.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5befaef-9c62-4400-961e-c1288368b6bd",
      "metadata": {
        "gather": {
          "logged": 1715035604225
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "target_shape_s1 = (522, 544, 1)\n",
        "target_shape_s2 = (522, 544, 3)\n",
        "\n",
        "train_grouped = group_by_scenario_and_type(train_data, target_shape_s1, target_shape_s2)\n",
        "test_grouped = group_by_scenario_and_type(test_data, target_shape_s1, target_shape_s2)\n",
        "validation_grouped = group_by_scenario_and_type(val_data, target_shape_s1, target_shape_s2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81cbd568-fb75-4f2c-9e91-1a3016c650a3",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Inspect Training Data Structure\n",
        "\n",
        "Define a function to inspect the structure of the training data.\n",
        "This helps in understanding the shape and type of each key in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "844608f6-8ca2-4585-a754-3c34d9ef103a",
      "metadata": {
        "gather": {
          "logged": 1715035608344
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def inspect_train_grouped_structure(train_grouped):\n",
        "    for scenario_id, data in train_grouped.items():\n",
        "        print(f\"Scenario ID: {scenario_id}\")\n",
        "        for key, value in data.items():\n",
        "            if isinstance(value, list) and len(value) > 0:\n",
        "                item = value[0]\n",
        "                if isinstance(item, dict):\n",
        "                    print(f\"  Key: {key}, Type: List of Dicts\")\n",
        "                    for sub_key in item:\n",
        "                        print(f\"    - Dict Key: {sub_key}, Shape/Type: {np.shape(item[sub_key])}\")\n",
        "                else:\n",
        "                    print(f\"  Key: {key}, Type: List of {type(item).__name__}\")\n",
        "            else:\n",
        "                print(f\"  Key: {key}, Type: {type(value).__name__}, Shape/Type: {np.shape(value)}\")\n",
        "        print()\n",
        "        \n",
        "# Assuming 'train_grouped' is defined\n",
        "inspect_train_grouped_structure(train_grouped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6f0a0a1-b4e1-472e-9caa-2de966cc301d",
      "metadata": {
        "gather": {
          "logged": 1715035610432
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def resize_image_to_shape(img, target_shape):\n",
        "    \"\"\"\n",
        "    Resizes an image to the target shape.\n",
        "    \"\"\"\n",
        "    if img.shape != target_shape:\n",
        "        img = resize(img, target_shape, preserve_range=True, anti_aliasing=True)\n",
        "    return img\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15848634-d6e2-43ad-bd7f-b18384bda543",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prepare Data for Model\n",
        "\n",
        "Define a function to prepare the data for the model.\n",
        "This function handles resizing and stacking the images for each scenario.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12e1f94-8f24-47ad-b670-7fdc84408a2b",
      "metadata": {
        "gather": {
          "logged": 1715035612312
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "def prepare_data_for_model(scenario_data, target_shape_s1, target_shape_s2):\n",
        "    sequences = []\n",
        "\n",
        "    for scenario_id, data in scenario_data.items():\n",
        "        s1_images, s2_images, s1_filenames, s2_filenames, labels, dates = [], [], [], [], [], []\n",
        "\n",
        "        # Gather all images and metadata into a unified list\n",
        "        all_data = []\n",
        "\n",
        "        # Process S1 images\n",
        "        for img_dict, fname, lbl, dt in zip(data['S1'], data['S1_filenames'], data['labels'], data['dates']):\n",
        "            if 'VH' in img_dict:\n",
        "                all_data.append({'image': img_dict['VH'], 'type': 'S1', 'filename': fname, 'label': lbl, 'date': dt})\n",
        "            else:\n",
        "                print(f\"Skipping S1 image {fname} as it does not contain a 'VH' key.\")\n",
        "\n",
        "        # Process S2 images\n",
        "        for img_dict, fname, lbl, dt in zip(data['S2'], data['S2_filenames'], data['labels'], data['dates']):\n",
        "            if 'rgb' in img_dict:\n",
        "                all_data.append({'image': img_dict['rgb'], 'type': 'S2', 'filename': fname, 'label': lbl, 'date': dt})\n",
        "            else:\n",
        "                print(f\"Skipping S2 image {fname} as it does not contain an 'rgb' key.\")\n",
        "\n",
        "        # Sort the unified list by date\n",
        "        all_data.sort(key=lambda x: x['date'])\n",
        "\n",
        "        # Separate the sorted data\n",
        "        for entry in all_data:\n",
        "            if entry['type'] == 'S1':\n",
        "                s1_images.append(resize_image_to_shape(entry['image'], target_shape_s1))\n",
        "                s1_filenames.append(entry['filename'])\n",
        "            else:  # entry['type'] == 'S2'\n",
        "                s2_images.append(resize_image_to_shape(entry['image'], target_shape_s2))\n",
        "                s2_filenames.append(entry['filename'])\n",
        "            labels.append(entry['label'])\n",
        "            dates.append(entry['date'])\n",
        "\n",
        "        # Stack images to form sequences\n",
        "        s1_images_np = np.stack(s1_images, axis=0) if s1_images else np.empty((0,) + target_shape_s1)\n",
        "        s2_images_np = np.stack(s2_images, axis=0) if s2_images else np.empty((0,) + target_shape_s2)\n",
        "\n",
        "        sequence_data = {\n",
        "            's1_images': s1_images_np,\n",
        "            's2_images': s2_images_np,\n",
        "            's1_filenames': s1_filenames,\n",
        "            's2_filenames': s2_filenames,\n",
        "            'labels': np.array(labels),\n",
        "            'dates': np.array(dates),\n",
        "            'scenario': scenario_id\n",
        "        }\n",
        "\n",
        "        sequences.append(sequence_data)\n",
        "        print(f\"Finished processing scenario {scenario_id} with {s1_images_np.shape[0]} S1 images and {s2_images_np.shape[0]} S2 images\")\n",
        "        print(f\"S1 Filenames: {s1_filenames}\")\n",
        "        print(f\"S2 Filenames: {s2_filenames}\")\n",
        "\n",
        "    return sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d55e3b5-7ee8-48c4-b365-ebe9d4d4af7a",
      "metadata": {
        "gather": {
          "logged": 1715035613366
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "train_sequences = prepare_data_for_model(train_grouped, target_shape_s1, target_shape_s2)\n",
        "val_sequences = prepare_data_for_model(validation_grouped, target_shape_s1, target_shape_s2)\n",
        "test_sequences = prepare_data_for_model(test_grouped, target_shape_s1, target_shape_s2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c680ec86-0f17-449c-b0e7-8ccd0808028d",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Build and Compile the Model\n",
        "\n",
        "Define the neural network architecture using Keras layers.\n",
        "The model uses ConvLSTM layers for both Sentinel-1 and Sentinel-2 inputs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c88bb6f-6002-4e41-ba5e-e9c7ccff9d78",
      "metadata": {
        "gather": {
          "logged": 1715035616674
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, Masking, ConvLSTM2D, BatchNormalization, Dense, Flatten, concatenate, GlobalAveragePooling2D, Embedding, TimeDistributed, GlobalAveragePooling3D, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Lambda\n",
        "import tensorflow.keras.backend as K\n",
        "import tensorflow as tf\n",
        "from keras.layers import Reshape\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from sklearn.utils import class_weight\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Define input shapes\n",
        "s1_input_shape = (None, 522, 544, 1)\n",
        "s2_input_shape = (None, 522, 544, 3)\n",
        "s1_input = Input(shape=s1_input_shape, name='S1_input')\n",
        "s2_input = Input(shape=s2_input_shape, name='S2_input')\n",
        "\n",
        "# S1 images processing branch\n",
        "s1_masked = Masking(mask_value=0.0)(s1_input)\n",
        "s1_branch = ConvLSTM2D(2, (3, 3), activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), padding=\"same\")(s1_masked)\n",
        "s1_branch = BatchNormalization()(s1_branch)\n",
        "s1_branch = Dropout(0.5)(s1_branch)\n",
        "s1_branch = ConvLSTM2D(2, (3, 3), activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), padding=\"same\")(s1_branch)\n",
        "s1_branch = Dropout(0.5)(s1_branch)\n",
        "\n",
        "# S2 images processing branch\n",
        "s2_masked = Masking(mask_value=0.0)(s2_input)\n",
        "s2_branch = ConvLSTM2D(2, (3, 3), activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), padding=\"same\")(s2_masked)\n",
        "s2_branch = BatchNormalization()(s2_branch)\n",
        "s2_branch = Dropout(0.5)(s2_branch)\n",
        "s2_branch = ConvLSTM2D(2, (3, 3), activation='relu', return_sequences=True, kernel_regularizer=l2(0.01), padding=\"same\")(s2_branch)\n",
        "s2_branch = Dropout(0.5)(s2_branch)\n",
        "\n",
        "s1_branch = Reshape(target_shape=(-1, s1_branch.shape[2] * s1_branch.shape[3] * s1_branch.shape[4]))(s1_branch)\n",
        "s2_branch = Reshape(target_shape=(-1, s2_branch.shape[2] * s2_branch.shape[3] * s2_branch.shape[4]))(s2_branch)\n",
        "\n",
        "# Combine outputs from both branches\n",
        "combined = concatenate([s1_branch, s2_branch], axis=-1)\n",
        "\n",
        "# Apply TimeDistributed to process each timestep\n",
        "final_layer = TimeDistributed(Dense(2, activation='relu'))(combined)\n",
        "output = TimeDistributed(Dense(1, activation='sigmoid'))(final_layer)\n",
        "\n",
        "learning_rate = 7e-6\n",
        "optimizer = SGD(learning_rate=learning_rate, clipnorm=1.0)\n",
        "\n",
        "# Create and compile the model\n",
        "model = Model(inputs=[s1_input, s2_input], outputs=output)\n",
        "model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Model summary\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47105bbb-4297-4787-a0ff-28768f1321e1",
      "metadata": {
        "gather": {
          "logged": 1714960161568
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "3d941cdf-07f1-4b8f-b28f-ebcc617e0aaa",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "TensorFlow and Keras Libraries\n",
        "Load necessary libraries for deep learning and data processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357d78a5-fd75-4ff0-bd88-03e59e716146",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Padding Image Sequences\n",
        "Define a function to pad image sequences to the same maximum length using numpy.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29fba8a0-af8f-4050-b661-628cf15cf2d8",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Padding Label Sequences\n",
        "Define a function to pad or truncate label sequences to match the given target length."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10648643-8195-4b54-b0f6-5ccd74b59d49",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Custom Weighted Binary Crossentropy\n",
        "Define a custom weighted binary crossentropy loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce03c9d7-b45c-41d2-9bb2-e7d2c8b5e08b",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Calculate Class Weights\n",
        "Define a function to calculate class weights for handling imbalanced data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f24859d6-413c-488d-9d12-fe63116066d8",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Train the Model\n",
        "Loop through the training epochs and train the model on the training data, while validating on the validation data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37a5e479-ec0e-4ee7-a3cf-b2817bde9545",
      "metadata": {
        "gather": {
          "logged": 1715036930115
        },
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": [
        "# Define a function to pad image sequences\n",
        "def pad_image_sequences(sequences, maxlen=None, target_shape=None):\n",
        "    \"\"\"Pad image sequences to the same maximum length using numpy.\"\"\"\n",
        "    padded_sequences = []\n",
        "    for seq in sequences:\n",
        "        padded_seq = []\n",
        "        for img in seq:\n",
        "            # Padding the image to match the target shape\n",
        "            padded_img = np.pad(img,\n",
        "                                pad_width=((0, target_shape[0] - img.shape[0]), \n",
        "                                           (0, target_shape[1] - img.shape[1]), \n",
        "                                           (0, target_shape[2] - img.shape[2])),\n",
        "                                mode='constant',\n",
        "                                constant_values=0)\n",
        "            padded_seq.append(padded_img)\n",
        "        # Pad or truncate the sequence to maxlen\n",
        "        if len(padded_seq) < maxlen:\n",
        "            padded_seq.extend([np.zeros(target_shape) for _ in range(maxlen - len(padded_seq))])\n",
        "        else:\n",
        "            padded_seq = padded_seq[:maxlen]\n",
        "        padded_sequences.append(padded_seq)\n",
        "    result = np.array(padded_sequences)\n",
        "    print(f\"pad_image_sequences result shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "# Define a function to pad label sequences\n",
        "def pad_labels_to_match_images(labels, target_length):\n",
        "    \"\"\"Pad or truncate label sequences to match the given target length.\"\"\"\n",
        "    adjusted_labels = []\n",
        "    for lbl in labels:\n",
        "        adjusted_lbl = lbl[:target_length] if len(lbl) > target_length else np.pad(lbl, (0, target_length - len(lbl)), 'constant')\n",
        "        adjusted_labels.append(adjusted_lbl)\n",
        "    result = np.array(adjusted_labels)\n",
        "    print(f\"pad_labels_to_match_images result shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "# Define a function for weighted binary crossentropy loss\n",
        "def weighted_binary_crossentropy(weights):\n",
        "    def loss(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        return tf.reduce_mean(-weights[1] * y_true * tf.math.log(y_pred + 1e-7) -\n",
        "                              weights[0] * (1 - y_true) * tf.math.log(1 - y_pred + 1e-7))\n",
        "    return loss\n",
        "\n",
        "# Define a function to calculate class weights\n",
        "def calculate_class_weights(y_train):\n",
        "    flat_y = np.concatenate(y_train).ravel()\n",
        "    weights = class_weight.compute_class_weight('balanced', classes=np.unique(flat_y), y=flat_y)\n",
        "    return {0: weights[0], 1: weights[1]}\n",
        "\n",
        "# Initialize training variables\n",
        "num_epochs = 10\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "val_losses = []\n",
        "val_accuracies = []\n",
        "\n",
        "target_shape_s1 = (522, 544, 1)\n",
        "target_shape_s2 = (522, 544, 3)\n",
        "\n",
        "# Prepare training data\n",
        "X_train_s1 = [seq['s1_images'] for seq in train_sequences]\n",
        "X_train_s2 = [seq['s2_images'] for seq in train_sequences]\n",
        "y_train = [seq['labels'] for seq in train_sequences]\n",
        "\n",
        "# Determine maximum sequence length\n",
        "max_len_train = max(max([len(seq) for seq in X_train_s1]),\n",
        "                    max([len(seq) for seq in X_train_s2]))\n",
        "\n",
        "# Pad S1 and S2 training data\n",
        "X_train_s1_padded = [pad_image_sequences([img], maxlen=max_len_train, target_shape=target_shape_s1) for img in X_train_s1]\n",
        "X_train_s2_padded = [pad_image_sequences([img], maxlen=max_len_train, target_shape=target_shape_s2) for img in X_train_s2]\n",
        "y_train_padded = [pad_labels_to_match_images([lbl], max_len_train) for lbl in y_train]\n",
        "\n",
        "# Prepare validation data\n",
        "X_val_s1 = [seq['s1_images'] for seq in val_sequences]\n",
        "X_val_s2 = [seq['s2_images'] for seq in val_sequences]\n",
        "y_val = [seq['labels'] for seq in val_sequences]\n",
        "\n",
        "# Determine maximum sequence length for validation data\n",
        "max_len_val = max(max([len(seq) for seq in X_val_s1]),\n",
        "                  max([len(seq) for seq in X_val_s2]))\n",
        "\n",
        "# Pad S1 and S2 validation data\n",
        "X_val_s1_padded = [pad_image_sequences([img], maxlen=max_len_val, target_shape=target_shape_s1) for img in X_val_s1]\n",
        "X_val_s2_padded = [pad_image_sequences([img], maxlen=max_len_val, target_shape=target_shape_s2) for img in X_val_s2]\n",
        "y_val_padded = [pad_labels_to_match_images([lbl], max_len_val) for lbl in y_val]\n",
        "\n",
        "# Calculate class weights based on true labels\n",
        "weights = calculate_class_weights(y_train_padded)\n",
        "print(\"Class weights:\", weights)\n",
        "\n",
        "# Compile the model with the weighted loss\n",
        "model.compile(optimizer='adam', loss=weighted_binary_crossentropy(weights), metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "    epoch_loss = []\n",
        "    epoch_accuracy = []\n",
        "    val_epoch_loss = []\n",
        "    val_epoch_accuracy = []\n",
        "\n",
        "    # Training loop\n",
        "    for s1_batch, s2_batch, labels_batch in zip(X_train_s1_padded, X_train_s2_padded, y_train_padded):\n",
        "        history = model.train_on_batch([s1_batch, s2_batch], labels_batch)\n",
        "        epoch_loss.append(history[0])\n",
        "        epoch_accuracy.append(history[1])\n",
        "\n",
        "    avg_loss = np.mean(epoch_loss)\n",
        "    avg_accuracy = np.mean(epoch_accuracy)\n",
        "    train_losses.append(avg_loss)\n",
        "    train_accuracies.append(avg_accuracy)\n",
        "    print(f\"Training - Epoch {epoch+1} - Loss: {avg_loss:.4f}, Accuracy: {avg_accuracy:.4f}\")\n",
        "\n",
        "    # Validation loop\n",
        "    for s1_batch, s2_batch, labels_batch in zip(X_val_s1_padded, X_val_s2_padded, y_val_padded):\n",
        "        val_history = model.test_on_batch([s1_batch, s2_batch], labels_batch)\n",
        "        val_epoch_loss.append(val_history[0])\n",
        "        val_epoch_accuracy.append(val_history[1])\n",
        "\n",
        "    val_avg_loss = np.mean(val_epoch_loss)\n",
        "    val_avg_accuracy = np.mean(val_epoch_accuracy)\n",
        "    val_losses.append(val_avg_loss)\n",
        "    val_accuracies.append(val_avg_accuracy)\n",
        "    print(f\"Validation - Epoch {epoch+1} - Loss: {val_avg_loss:.4f}, Accuracy: {val_avg_accuracy:.4f}\")\n",
        "\n",
        "    # Adjust weights after the first epoch\n",
        "    if epoch == 0:\n",
        "        weights = calculate_class_weights(y_train_padded)\n",
        "        print(\"Adjusted weights after first epoch:\", weights)\n",
        "        model.compile(optimizer='adam', loss=weighted_binary_crossentropy(weights), metrics=['accuracy'])\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"trainedfinal_model.h5\")\n",
        "\n",
        "# Plot training and validation metrics\n",
        "epochs = list(range(1, num_epochs + 1))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(epochs, train_losses, label='Training Loss')\n",
        "plt.plot(epochs, val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss over epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs, val_accuracies, label='Validation Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy over epochs')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"training_validation_plot_final.png\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09fae64d-d59c-4206-871f-e9ef9244203d",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "9dec9099-9761-4ae5-badf-72327b24cc03",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Same preprocess step to fix dimension as did it with train and validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c758fd06",
      "metadata": {
        "gather": {
          "logged": 1715058715278
        }
      },
      "outputs": [],
      "source": [
        "# Prepare test data\n",
        "X_test_s1 = [seq['s1_images'] for seq in test_sequences]\n",
        "X_test_s2 = [seq['s2_images'] for seq in test_sequences]\n",
        "y_test = [seq['labels'] for seq in test_sequences]\n",
        "\n",
        "max_len_test = max(max([len(seq) for seq in X_test_s1]), max([len(seq) for seq in X_test_s2]))\n",
        "\n",
        "target_shape_s1 = (522, 544, 1)\n",
        "target_shape_s2 = (522, 544, 3)\n",
        "\n",
        "# Pad test data\n",
        "X_test_s1_padded = [pad_image_sequences([img], maxlen=max_len_test, target_shape=target_shape_s1) for img in X_test_s1]\n",
        "X_test_s2_padded = [pad_image_sequences([img], maxlen=max_len_test, target_shape=target_shape_s2) for img in X_test_s2]\n",
        "y_test_padded = [pad_labels_to_match_images([lbl], max_len_test) for lbl in y_test]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bcd0f080-7900-41ff-b8ef-2e65203fc3dd",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Making Predictions\n",
        "Make predictions on the test data using the trained model. y_test results are padded to the length of S1 to match dimensionsionality"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbb3de09",
      "metadata": {
        "gather": {
          "logged": 1715059400633
        }
      },
      "outputs": [],
      "source": [
        "predictions = []\n",
        "for s1_batch, s2_batch in zip(X_test_s1, X_test_s2):\n",
        "    # Determine the maximum length\n",
        "    max_len = max(len(s1_batch), len(s2_batch))\n",
        "    \n",
        "    # Pad s1_batch and s2_batch to the maximum length\n",
        "    s1_batch_padded = pad_image_sequences([s1_batch], maxlen=max_len, target_shape=(522, 544, 1))\n",
        "    s2_batch_padded = pad_image_sequences([s2_batch], maxlen=max_len, target_shape=(522, 544, 3))\n",
        "    \n",
        "    # Predict\n",
        "    prediction = model.predict([s1_batch_padded, s2_batch_padded])\n",
        "    predictions.append(prediction)\n",
        "\n",
        "predicted_labels = [(pred > 0.5).astype(int).flatten() for pred in predictions]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "72435389",
      "metadata": {
        "gather": {
          "logged": 1715059410780
        }
      },
      "outputs": [],
      "source": [
        "print(predicted_labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eae2b2a2-52d0-48f4-a4f4-b348b055693e",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Calculate Metrics\n",
        "Calculate accuracy, precision, recall, and F1 score for each test scenario."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c83c4285-e689-4b1b-a2a7-9fc52533c8cb",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Average Metrics\n",
        "Calculate and print the average metrics across all test scenarios.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e223b4f9",
      "metadata": {
        "gather": {
          "logged": 1715059543378
        }
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "# Calculate metrics for each scenario\n",
        "# Calculate metrics for each scenario\n",
        "accuracy_scores = []\n",
        "precision_scores = []\n",
        "recall_scores = []\n",
        "f1_scores = []\n",
        "\n",
        "for i in range(len(y_test_padded)):\n",
        "    y_true = y_test_padded[i].flatten()\n",
        "    y_pred = predicted_labels[i]\n",
        "    \n",
        "    # Align lengths\n",
        "    max_len = max(len(y_true), len(y_pred))\n",
        "    if len(y_true) < max_len:\n",
        "        y_true = np.pad(y_true, (0, max_len - len(y_true)), 'constant')\n",
        "    if len(y_pred) < max_len:\n",
        "        y_pred = np.pad(y_pred, (0, max_len - len(y_pred)), 'constant')\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    precision = precision_score(y_true, y_pred)\n",
        "    recall = recall_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred)\n",
        "    \n",
        "    accuracy_scores.append(accuracy)\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "    # Print the metrics for each scenario\n",
        "    print(f\"Scenario {i + 1}:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print()\n",
        "\n",
        "# Calculate average metrics across scenarios\n",
        "avg_accuracy = sum(accuracy_scores) / len(accuracy_scores)\n",
        "avg_precision = sum(precision_scores) / len(precision_scores)\n",
        "avg_recall = sum(recall_scores) / len(recall_scores)\n",
        "avg_f1 = sum(f1_scores) / len(f1_scores)\n",
        "\n",
        "# Print average metrics\n",
        "print(\"Average Metrics:\")\n",
        "print(f\"Average Accuracy: {avg_accuracy:.4f}\")\n",
        "print(f\"Average Precision: {avg_precision:.4f}\")\n",
        "print(f\"Average Recall: {avg_recall:.4f}\")\n",
        "print(f\"Average F1 Score: {avg_f1:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60aa010a-b1b5-452d-b65e-cbda83b32405",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python38-azureml"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "microsoft": {
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      },
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
